{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2868f5ba",
   "metadata": {},
   "source": [
    "# 0. Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf00b6",
   "metadata": {},
   "source": [
    "Import the needed packages. Nltk may needs to be downloaded. Please use pip install for the packages you currently do not have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of nltk library for further use\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import requests\n",
    "# downloading of the packages in nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80085d4b",
   "metadata": {},
   "source": [
    "In case your current working directory does not have enough storage, you can change to another local directory. In our case we use \"D:/Dataset/\", please feel free to change that to the directory you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1033d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory if needed\n",
    "# os.chdir(\"D:/Dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fee1a",
   "metadata": {},
   "source": [
    "Get the dataset file from github. The original dataset can be found at: https://doi.org/10.5281/zenodo.5591908."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2a320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text for testing of preprocessing\n",
    "path_here = 'https://media.githubusercontent.com/media/patrickzw7/TextMiningProject/master/'\n",
    "f = requests.get(path_here + 'FT-en-zh.txt')\n",
    "f.encoding = f.apparent_encoding\n",
    "text_org = f.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0135b",
   "metadata": {},
   "source": [
    "Generate the dataset files using the original raw dataset file to your local directory. Some basic text cleaning is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08683ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the different texts by the new lines\n",
    "text_all = text_org.split(\"\\n\")\n",
    "\n",
    "# try to make two new directories on your local working directory\n",
    "try:\n",
    "    os.mkdir(\"DatasetEnglish/\")\n",
    "    os.mkdir(\"DatasetChinese/\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# try to do basic text cleaning and write the texts into seperate Chinese and English files for each text\n",
    "for i in range(len(text_all) - 1):\n",
    "    text = text_all[i]\n",
    "    text = re.sub(\"&#\\d\\d\\d\\d\", \"\", text)\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    texts = text.split(\";\")\n",
    "    text_name = texts[0]\n",
    "    text_time = texts[1]\n",
    "    text_en_title = texts[2]\n",
    "    text_zh_title = texts[3]\n",
    "    text_en_text = texts[5].split(\">\")[-1]\n",
    "    text_zh_text = texts[6]\n",
    "    if not os.path.exists(f'DatasetEnglish/{text_name}.txt'):\n",
    "        with open(f'DatasetEnglish/{text_name}.txt', 'a', encoding = \"utf-8\") as f:\n",
    "            f.write(text_en_text)\n",
    "    if not os.path.exists(f'DatasetChinese/{text_name}.txt'):\n",
    "        with open(f'DatasetChinese/{text_name}.txt', 'a', encoding = \"utf-8\") as f:\n",
    "            f.write(text_zh_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0ddb6",
   "metadata": {},
   "source": [
    "# 1. Preprocessing for English Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be4777",
   "metadata": {},
   "source": [
    "Read the sample data (the first file) for English dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f880dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London must not be complacent about its continuing pre-eminence as a financial centre but the availability of skilled staff, flexible labour markets and comparatively light regulation now make the City the best place for global institutions to do business, according to the business district’s governing body.  The Corporation of London’s latest research into the ranking of financial centres, published yesterday, also suggests China’s increasing economic power makes it likely to host any new global financial services centre in future years.  Compared with a previous study in 2003, London and New York had moved even further ahead of Frankfurt and Paris when assessed according to criteria seen as important for competitiveness by 400 individuals working in the industry.  This year London was slightly more likely to be named as the chosen location for transactions requiring a global financial centre, whereas New York came top in the previous survey. But the report said the two cities would continue to be “neck and neck”.   “Although Paris and Frankfurt are extremely important regional financial centres . . . the pre-eminence of the City and Wall Street is if anything even more pronounced than before,” said Michael Snyder, the corporation’s chairman of policy and resources.  No one consulted in the survey believed London and New York would lose their positions as global financial centres within the next 10 years. But respondents were divided over whether there was scope for a third one. If one did develop, most felt it would be in China, probably in Shanghai. Respondents did not believe Tokyo could recover its previous importance because of bureau-cracy and poor regulation.   The report’s authors warned that there was no room for either London or New York to be complacent about strengths that were often brought about by historical factors, when the future could bring fresh competitive challenges.   For example, the survey suggested corporate and personal tax regimes were likely to be of greater concern to the financial sector over the next three years, although most were finding ways to cope at the moment.  One respondent said the City had a shortage of skills in derivatives operations. And the survey showed that London was seen as an expensive location.  But concerns that a relatively light regulatory burden was getting heavier were offset by the flexibility and quality of staff in London as well as by a view that regulation was more of a problem elsewhere in the European Union.   Another potential threat to jobs in the finance sector in the two dominant cities was technology and outsourcing to cheaper cities, particularly in Asia.  But one head of trading at a London-based investment bank reflected the views of most respondents in explaining why off-shore and outsourced operations would not challenge London and New York as the best places to site central operations.  “Nobody can move liquidity unilaterally and so once a global centre such as London or New York has been established it is virtually impossible to move,” he said.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the text for testing of preprocessing\n",
    "path_here = r'DatasetEnglish/'\n",
    "f = open(path_here + '1001571.txt', encoding = 'utf-8')\n",
    "text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425f201",
   "metadata": {},
   "source": [
    "## 1a. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade017f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['London must not be complacent about its continuing pre-eminence as a financial centre but the availability of skilled staff, flexible labour markets and comparatively light regulation now make the City the best place for global institutions to do business, according to the business district’s governing body.']\n",
      "['The Corporation of London’s latest research into the ranking of financial centres, published yesterday, also suggests China’s increasing economic power makes it likely to host any new global financial services centre in future years.']\n",
      "['Compared with a previous study in 2003, London and New York had moved even further ahead of Frankfurt and Paris when assessed according to criteria seen as important for competitiveness by 400 individuals working in the industry.']\n",
      "['This year London was slightly more likely to be named as the chosen location for transactions requiring a global financial centre, whereas New York came top in the previous survey.']\n",
      "['But the report said the two cities would continue to be “neck and neck”.']\n",
      "['“Although Paris and Frankfurt are extremely important regional financial centres .']\n",
      "['.']\n",
      "['.']\n",
      "['the pre-eminence of the City and Wall Street is if anything even more pronounced than before,” said Michael Snyder, the corporation’s chairman of policy and resources.']\n",
      "['No one consulted in the survey believed London and New York would lose their positions as global financial centres within the next 10 years.']\n",
      "['But respondents were divided over whether there was scope for a third one.']\n",
      "['If one did develop, most felt it would be in China, probably in Shanghai.']\n",
      "['Respondents did not believe Tokyo could recover its previous importance because of bureau-cracy and poor regulation.']\n",
      "['The report’s authors warned that there was no room for either London or New York to be complacent about strengths that were often brought about by historical factors, when the future could bring fresh competitive challenges.']\n",
      "['For example, the survey suggested corporate and personal tax regimes were likely to be of greater concern to the financial sector over the next three years, although most were finding ways to cope at the moment.']\n",
      "['One respondent said the City had a shortage of skills in derivatives operations.']\n",
      "['And the survey showed that London was seen as an expensive location.']\n",
      "['But concerns that a relatively light regulatory burden was getting heavier were offset by the flexibility and quality of staff in London as well as by a view that regulation was more of a problem elsewhere in the European Union.']\n",
      "['Another potential threat to jobs in the finance sector in the two dominant cities was technology and outsourcing to cheaper cities, particularly in Asia.']\n",
      "['But one head of trading at a London-based investment bank reflected the views of most respondents in explaining why off-shore and outsourced operations would not challenge London and New York as the best places to site central operations.']\n",
      "['“Nobody can move liquidity unilaterally and so once a global centre such as London or New York has been established it is virtually impossible to move,” he said.']\n"
     ]
    }
   ],
   "source": [
    "# we use sent_tokenize from nltk to do the sentence segmentation\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# print the segmented sentences\n",
    "for i in sentences:\n",
    "    print([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533ea59",
   "metadata": {},
   "source": [
    "## 1b. Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db94c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "must\n",
      "not\n",
      "be\n",
      "complacent\n",
      "about\n",
      "its\n",
      "continuing\n",
      "pre-eminence\n",
      "as\n"
     ]
    }
   ],
   "source": [
    "# we use word_tokenize from nltk to do the word segmentation\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# print the segmented words\n",
    "for i in range(10):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92e558",
   "metadata": {},
   "source": [
    "## 1c.Stop Words Removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff30dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London',\n",
       " 'must',\n",
       " 'complacent',\n",
       " 'continuing',\n",
       " 'pre-eminence',\n",
       " 'financial',\n",
       " 'centre',\n",
       " 'availability',\n",
       " 'skilled',\n",
       " 'staff']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use word.lower() to avoid the first word in text, like \"The\", \n",
    "# not being removed as only \"the\" is included in stopwords \n",
    "tokens_stop_remove = [word for word in tokens if not word.lower() in set(stopwords.words('english'))]\n",
    "\n",
    "# print the tokens after removing the stop words\n",
    "tokens_stop_remove[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79935509",
   "metadata": {},
   "source": [
    "## 1d. Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a759bab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['london',\n",
       "  'must',\n",
       "  'not',\n",
       "  'be',\n",
       "  'complacent',\n",
       "  'about',\n",
       "  'it',\n",
       "  'continuing',\n",
       "  'pre-eminence',\n",
       "  'a'],\n",
       " ['london',\n",
       "  'must',\n",
       "  'not',\n",
       "  'be',\n",
       "  'complac',\n",
       "  'about',\n",
       "  'it',\n",
       "  'continu',\n",
       "  'pre-emin',\n",
       "  'as']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the lemmatizer library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#define the lemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "#define a list to store the words after lemmatizing\n",
    "tokens_lemmatized = []\n",
    "#do a loop to lemmatize the words in tokens\n",
    "for i in range(int(len(tokens))):\n",
    "    #append the lemmatized version for each word into the new list\n",
    "    tokens_lemmatized.append(lemma.lemmatize(tokens[i].lower()))\n",
    "#import the stemming library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#define the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "#define a list to store the words after stemming\n",
    "tokens_stemmed = []\n",
    "#do a loop to stem the words in tokens\n",
    "for i in range(int(len(tokens))):\n",
    "    #append the stemmed version for each word into the new list\n",
    "    tokens_stemmed.append(stemmer.stem(tokens[i].lower()))\n",
    "#print out the lemmatized and stemmed result\n",
    "[tokens_lemmatized[:10],tokens_stemmed[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb6e7d",
   "metadata": {},
   "source": [
    "## 1e. Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f80e08cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['London', 'pre-eminence', 'financial centre', 'availability', 'skilled staff', 'flexible labour', 'light regulation', 'City', 'place', 'business', 'business', 'district', '', 's', 'body', 'Corporation', 'London', 'research', 'ranking', 'yesterday', 'China', 'economic power', 'centre', 'previous study', 'London', 'New York', 'Frankfurt', 'Paris', 'competitiveness', 'industry', 'year', 'London', 'chosen', 'location', 'global financial centre', 'New York', 'previous survey', 'report', '“ neck', 'neck', '”', 'Paris', 'Frankfurt', 'pre-eminence', 'City', 'Wall Street', 'anything', '”', 'Michael Snyder', 'corporation', '', 's', 'chairman', 'policy', 'one', 'survey', 'London', 'New York', 'scope', 'China', 'Shanghai', 'Tokyo', 'previous importance', 'bureau-cracy', 'poor regulation', 'report', '', 'room', 'London', 'New York', 'future', 'example', 'survey', 'personal tax', 'concern', 'financial sector', 'moment', 'respondent', 'City', 'shortage', 'survey', 'London', 'expensive location', 'light regulatory burden', 'flexibility', 'quality', 'staff', 'London', 'view', 'regulation', 'problem', 'European Union', 'potential threat', 'finance', 'sector', 'technology', 'Asia', 'head', 'trading', 'London-based investment', 'bank', 'off-shore', 'London', 'New York', 'site', 'Nobody', 'liquidity', 'global centre', 'London', 'New York']\n"
     ]
    }
   ],
   "source": [
    "# import of the RegexpParser library from nltk\n",
    "from nltk import RegexpParser\n",
    "# definition of the grammer\n",
    "grammer = r\"\"\"\n",
    "  NP: {<PP\\$>?<JJ>*<NN>}   \n",
    "      {<NNP>+}                \n",
    "\"\"\"\n",
    "# definition of the parser\n",
    "parser = RegexpParser(grammer)\n",
    "# tagging for tokens\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "# use the parser to parse the tagged words\n",
    "parse = parser.parse(tagged)\n",
    "# define an empty list to put in the noun phrases\n",
    "entity = []\n",
    "for i in parse:\n",
    "    # define an empty string variable\n",
    "    entity_str = \"\"\n",
    "    # check if it is a noun phrase that suits the requirements\n",
    "    if type(i) == Tree:\n",
    "        for token,pos in i.leaves():\n",
    "            # put the words in the named entity into the string\n",
    "            entity_str += str(token)\n",
    "            # add an empty space between each word\n",
    "            entity_str += \" \"\n",
    "        # replace the incorrect signs\n",
    "        entity_str = entity_str.replace(\"’\", \"\")\n",
    "        # strip the empty space in the string\n",
    "        entity_str = entity_str.rstrip().lstrip()\n",
    "        # append the final string to the named entity list\n",
    "        # if entity_str.split()[0] != \"s\":\n",
    "        entity.append(entity_str)\n",
    "print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57701675",
   "metadata": {},
   "source": [
    "## 1f. Named Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585f6fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('must', 'MD')\n",
      "('not', 'RB')\n",
      "('be', 'VB')\n",
      "('complacent', 'JJ')\n",
      "('about', 'IN')\n",
      "('its', 'PRP$')\n",
      "('continuing', 'VBG')\n",
      "('pre-eminence', 'NN')\n",
      "('as', 'IN')\n"
     ]
    }
   ],
   "source": [
    "# do the pos tagging process, get the identity for each word after stop words removing\n",
    "chunks = nltk.ne_chunk(tagged)\n",
    "# create an empty list to store the named entities\n",
    "entity2 = []\n",
    "for i in chunks:\n",
    "    # define an empty string variable\n",
    "    entity_str = \"\"\n",
    "    # check if the chunk is a named entity\n",
    "    if type(i) == Tree:\n",
    "        for token,pos in i.leaves():\n",
    "            # put the words in the named entity into the string\n",
    "            entity_str += str(token)\n",
    "            # add an empty space between each word\n",
    "            entity_str += \" \"\n",
    "        # strip the empty space in the string\n",
    "        entity_str = entity_str.rstrip()\n",
    "        # append the final string to the named entity list\n",
    "        entity2.append(entity_str)\n",
    "        \n",
    "for i in chunks[:10]:\n",
    "    if type(i) != Tree:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127222d3",
   "metadata": {},
   "source": [
    "## 1g. Overall Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c48e3",
   "metadata": {},
   "source": [
    "\"python -m spacy download en_core_web_sm\" is needed for anaconda prompt (or other terminals) for the en_core_web_sm from spacy.The stopword list for English is from kaggle: https://www.kaggle.com/datasets/rtatman/stopword-lists-for-19-languages?resource=download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e2899e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#overall preprocessing process(together with the word embedding part in the next cell)\n",
    "import time\n",
    "#import of the needed packages\n",
    "#if not installed, pip install them\n",
    "import os\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "#path where we store the txt files\n",
    "#change the path to your own path where you store your own test files\n",
    "path = r\"DatasetEnglish/\"\n",
    "#define the files as all files under the certain path\n",
    "files = os.listdir(path)\n",
    "\n",
    "# make a work directory\n",
    "try:\n",
    "    os.mkdir(\"Work/\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#initialize the English space\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "sp.max_length = 4000000\n",
    "start = time.time()\n",
    "#initialize match for numbers\n",
    "nums_compiled = re.compile(r\"[0-9]+\")\n",
    "#initialize match for punctuations\n",
    "puncs = punctuation + u'.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "puncs_compiled = re.compile(r\"[{}]+\".format(puncs))\n",
    "#define the stopwords\n",
    "stopwords_file = r'StopwordsEnglish.txt'\n",
    "#get the list of stopwords\n",
    "stopwords_here = set([line.strip() for line in open(stopwords_file, encoding=\"utf-8\").readlines()])\n",
    "\n",
    "#see which files are processed\n",
    "#files_processed = []\n",
    "\n",
    "#go through the files in files\n",
    "if os.path.isfile(\"Work/wordsEnglish.txt\") == False:\n",
    "    for file in files:\n",
    "        #append file to files processed\n",
    "        #files_processed.append(file)\n",
    "        \n",
    "        #print file if file is being processed\n",
    "        print(file)\n",
    "        \n",
    "        #check if the file is a txt file\n",
    "        if file.endswith(\".txt\"):\n",
    "            #if txt file satisfied, open it and read it into the text space\n",
    "            text = open(path + file, errors = 'ignore', encoding = 'utf-8').read() + '\\n\\n'\n",
    "\n",
    "            #preprocessing: change all words to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            #preprocessing: remove the punctuations\n",
    "            text = puncs_compiled.sub(\" \", text)\n",
    "\n",
    "            #preprocessing: remove the numbers \n",
    "            text = nums_compiled.sub(\" \", text)\n",
    "            \n",
    "            #preprocessing: word segmentation\n",
    "            #we use word_tokenize from nltk to do the word segmentation\n",
    "            tokens_org = nltk.word_tokenize(text)\n",
    "\n",
    "            #preprocessing: remove the stop words \n",
    "            #define a list to store the words that are not stop words\n",
    "            tokens = []\n",
    "            #go through the words in tokens\n",
    "            for i in range(0, int(len(tokens_org))):\n",
    "                #if not a stop word, append it to the new list\n",
    "                if tokens_org[i] not in stopwords_here:\n",
    "                    tokens.append(tokens_org[i])\n",
    "            \n",
    "            #preprocessing: put the words after removing stop-words to new text space\n",
    "            text_new = \"\"\n",
    "            for i in tokens:\n",
    "                text_new += i\n",
    "                text_new += \" \"\n",
    "            text_new = text_new.rstrip()\n",
    "\n",
    "            #preprocessing: lemmatizing and named entity removing\n",
    "            doc = sp(text_new)\n",
    "            #initialize a string to store the final text after lemmatize\n",
    "            text_final = \"\"\n",
    "            for token in doc:\n",
    "                if not token.ent_type_:\n",
    "                    #store the token after lemmatize, like \"good\" for the token \"better\"\n",
    "                    lemma_token = token.lemma_\n",
    "                    #add the token after lemmatize to the final string storing the lemmatized token\n",
    "                    text_final += lemma_token\n",
    "                    #add a blank space between each word\n",
    "                    text_final += \" \"\n",
    "                \n",
    "            #open a new file and write the words into it\n",
    "            #change the path to your own path where you want to store the words.txt file \n",
    "            with open (\"Work/wordsEnglish.txt\", 'a', encoding = 'utf-8') as f:\n",
    "                #a single line of words for each file \n",
    "                text_write = text_final + '\\n'\n",
    "                #write the words into the file\n",
    "                f.write(text_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d641d7",
   "metadata": {},
   "source": [
    "## 1h. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dea9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83183753\n"
     ]
    }
   ],
   "source": [
    "#import of word2vec library\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "#load the text file word corpus\n",
    "#change the path to your own path where you store the words.txt file generated above\n",
    "corpus = word2vec.Text8Corpus(\"Work\\\\wordsEnglish.txt\")\n",
    "#initialize the model, also the word embedding process\n",
    "model = Word2Vec(corpus, vector_size=100, window = 5, \\\n",
    "                              min_count = 2, workers = 4)\n",
    "#save the model into a file\n",
    "model.save(\"models.bin\")\n",
    "#load the model\n",
    "gensim.models.Word2Vec.load(\"models.bin\")\n",
    "#get the similarity for two words using the model\n",
    "print(model.wv.similarity(\"cat\", \"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0976a",
   "metadata": {},
   "source": [
    "get the 10 most similar words for some word, like \"science\" in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b8149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mathematics', 0.7780669927597046), ('physics', 0.7777588367462158), ('scientific', 0.7601035833358765), ('sciences', 0.7598090171813965), ('advancement', 0.7573344707489014), ('mit', 0.7559366822242737), ('biology', 0.7558315992355347), ('fiction', 0.7421140074729919), ('scientist', 0.7417101263999939), ('excellence', 0.7403338551521301)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2c2f3",
   "metadata": {},
   "source": [
    "# 2. Preprocessing for Chinese Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933646",
   "metadata": {},
   "source": [
    "Read the sample data from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9c8f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'伦敦商业区的管理机构表示，尽管伦敦一直都是卓越的金融中心，但它决不能自满，不过伦敦拥有熟练的员工，灵活的劳动力市场，以及比较宽松的监管，伦敦金融城因此成为目前全球机构经营的最佳地点。  昨天，伦敦金融城当局(Corporation of London)公布了对各金融中心排名的最新研究，该研究还表示，由于中国的经济力量日益增强，未来数年，很可能出现新的全球金融服务中心。  按照400名业内人士指定的评估标准(这些标准被视为对竞争力很重要)，相比之前2003年的研究，伦敦和纽约相比法兰克福和巴黎的领先优势更大了。  今年，对那些需要全球金融中心的交易而言，伦敦成为首选地的可能性稍大，而之前的调查是纽约位居榜首。但这次的报告说，两城市将继续“并驾齐驱”。  “尽管巴黎与法兰克福是极为重要的区域金融中心……但相比过去，伦敦金融城与华尔街的声誉却更加卓著，”金融城当局政策与资源委员会主席迈克尔 斯奈德(Michael Snyder)说。  在受访者中，没人认为伦敦和纽约会在10年内失去全球金融中心的地位。但对于是否会出现第三个全球金融中心，受访者的意见存在分歧。多数人觉得，如果真的出现第三个，将会在中国，很可能是上海。受访者认为，由于官僚作风和监管不力，东京不可能恢复像以前那样的重要地位。  报告作者警告说，伦敦与纽约的实力通常都是历史因素造成的，因此它们没有理由自满，因为将来可能出现全新的有力挑战。  例如，该调查表示，未来三年，企业与个人税收制度很可能成为金融业更大的担忧，尽管多数都在寻找应对办法。  一名受访者表示，金融城的衍生品业务技能不足。而且调查显示，人们认为伦敦是个成本很高的地方。  但有人担心，伦敦相对宽松的监管负担正在加重，不过由于伦敦的雇员具有灵活性，素质优异，再加上人们认为欧盟其它地区的监管更成问题，因此这种担心也就减轻了。  技术和外包，尤其是向成本更低廉的亚洲城市外包，给两座头号城市金融业职位带来另一个威胁。  但在一家总部设在伦敦的投资银行，一名交易负责人表达了多数受访者的观点，他解释了为何离岸和外包业务不会对伦敦和纽约构成挑战，它们仍是中心业务选的最佳选址。  “没人能单方面转移流动资产，因此一旦建立了伦敦或纽约这样的全球中心，要搬迁几乎是不可能的，”他说道。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_here = r'DatasetChinese/'\n",
    "f = open(path_here + '1001571.txt', encoding = 'utf-8')\n",
    "text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f770e6",
   "metadata": {},
   "source": [
    "## 2a. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0a71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['伦敦商业区的管理机构表示，尽管伦敦一直都是卓越的金融中心，但它决不能自满，不过伦敦拥有熟练的员工，灵活的劳动力市场，以及比较宽松的监管，伦敦金融城因此成为目前全球机构经营的最佳地点']\n",
      "['  昨天，伦敦金融城当局(Corporation of London)公布了对各金融中心排名的最新研究，该研究还表示，由于中国的经济力量日益增强，未来数年，很可能出现新的全球金融服务中心']\n",
      "['  按照400名业内人士指定的评估标准(这些标准被视为对竞争力很重要)，相比之前2003年的研究，伦敦和纽约相比法兰克福和巴黎的领先优势更大了']\n",
      "['  今年，对那些需要全球金融中心的交易而言，伦敦成为首选地的可能性稍大，而之前的调查是纽约位居榜首']\n",
      "['但这次的报告说，两城市将继续“并驾齐驱”']\n",
      "['  “尽管巴黎与法兰克福是极为重要的区域金融中心……但相比过去，伦敦金融城与华尔街的声誉却更加卓著，”金融城当局政策与资源委员会主席迈克尔 斯奈德(Michael Snyder)说']\n",
      "['  在受访者中，没人认为伦敦和纽约会在10年内失去全球金融中心的地位']\n",
      "['但对于是否会出现第三个全球金融中心，受访者的意见存在分歧']\n",
      "['多数人觉得，如果真的出现第三个，将会在中国，很可能是上海']\n",
      "['受访者认为，由于官僚作风和监管不力，东京不可能恢复像以前那样的重要地位']\n",
      "['  报告作者警告说，伦敦与纽约的实力通常都是历史因素造成的，因此它们没有理由自满，因为将来可能出现全新的有力挑战']\n",
      "['  例如，该调查表示，未来三年，企业与个人税收制度很可能成为金融业更大的担忧，尽管多数都在寻找应对办法']\n",
      "['  一名受访者表示，金融城的衍生品业务技能不足']\n",
      "['而且调查显示，人们认为伦敦是个成本很高的地方']\n",
      "['  但有人担心，伦敦相对宽松的监管负担正在加重，不过由于伦敦的雇员具有灵活性，素质优异，再加上人们认为欧盟其它地区的监管更成问题，因此这种担心也就减轻了']\n",
      "['  技术和外包，尤其是向成本更低廉的亚洲城市外包，给两座头号城市金融业职位带来另一个威胁']\n",
      "['  但在一家总部设在伦敦的投资银行，一名交易负责人表达了多数受访者的观点，他解释了为何离岸和外包业务不会对伦敦和纽约构成挑战，它们仍是中心业务选的最佳选址']\n",
      "['  “没人能单方面转移流动资产，因此一旦建立了伦敦或纽约这样的全球中心，要搬迁几乎是不可能的，”他说道']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "sentences = re.split('(。|！|\\!|\\.|？|\\?)',text)\n",
    "for i in sentences:\n",
    "    if i != \"。\":\n",
    "        print([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a8204",
   "metadata": {},
   "source": [
    "## 2b. Words Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f09beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.854 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['伦敦', '商业区', '的', '管理机构', '表示', '，', '尽管', '伦敦', '一直', '都']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = jieba.lcut(text)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f8a48",
   "metadata": {},
   "source": [
    "## 2c. Stop-words Removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a499d8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['打开天窗说亮话',\n",
       " '到目前为止',\n",
       " '赶早不赶晚',\n",
       " '常言说得好',\n",
       " '何乐而不为',\n",
       " '毫无保留地',\n",
       " '由此可见',\n",
       " '这就是说',\n",
       " '这么点儿',\n",
       " '综上所述']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the stopwords through a file of previous-generated stopwords\n",
    "f_stop = open('StopwordsChinese.txt', encoding = 'utf-8')\n",
    "text_stop = f_stop.read()\n",
    "stopwords = text_stop.split(\"\\n\")\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb68f67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['伦敦', '商业区', '管理机构', '表示', '，', '伦敦', '卓越', '金融中心', '，', '决不能']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stop_remove = [word for word in words if not word in stopwords]\n",
    "\n",
    "# print the tokens after removing the stop words\n",
    "tokens_stop_remove[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c149f",
   "metadata": {},
   "source": [
    "## 2d. Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b168fb",
   "metadata": {},
   "source": [
    "This part is ignored as Chinese words are hard to do actual lemmatization because of the particularity of the language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcceacff",
   "metadata": {},
   "source": [
    "## 2e. Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750c3a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pair('伦敦', 'ns'),\n",
       " pair('商业区', 'n'),\n",
       " pair('的', 'uj'),\n",
       " pair('管理机构', 'n'),\n",
       " pair('表示', 'v'),\n",
       " pair('，', 'x'),\n",
       " pair('尽管', 'c'),\n",
       " pair('伦敦', 'ns'),\n",
       " pair('一直', 'd'),\n",
       " pair('都', 'd')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words_type = pseg.lcut(text)\n",
    "words_type[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e03799",
   "metadata": {},
   "source": [
    "## 2f. Named Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb4a77e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['华尔街', '中国', '城市', '东京', '伦敦', '上海', '纽约', '亚洲', '巴黎']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty list to store the named entities\n",
    "entity2 = []\n",
    "for i in pseg.cut(text):\n",
    "    if tuple(i)[1] == ('ns' or 'nz' or 'nrt' or 'nr'):\n",
    "        entity2.append(tuple(i)[0])\n",
    "entity2 = list(set(entity2))\n",
    "entity2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea118b",
   "metadata": {},
   "source": [
    "## 2g. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31372c",
   "metadata": {},
   "source": [
    "TF-IDF is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e5f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tf-idf to get the 5 most representative words for each test file \n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# append text to corpus\n",
    "corpus = []\n",
    "corpus.append(text)\n",
    "\n",
    "#initialize of the tf-idf transformer\n",
    "transformer = TfidfTransformer()\n",
    "#initialize of the vectorizer of words\n",
    "vectorizer = CountVectorizer()\n",
    "#vectorize the corpus\n",
    "vector = vectorizer.fit_transform(corpus)\n",
    "#calculate the tf-idf for all words\n",
    "tfidf = transformer.fit_transform(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa05fda",
   "metadata": {},
   "source": [
    "## 2h. Overall Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "891d839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall preprocessing process(together with the word embedding part in the next cell)\n",
    "\n",
    "#import of the needed packages\n",
    "#if not installed, pip install them\n",
    "import os\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "#path where we store the txt files\n",
    "#change the path to your own path where you store your own test files\n",
    "path = r\"DatasetChinese/\"\n",
    "#define the files as all files under the certain path\n",
    "files = os.listdir(path)\n",
    "\n",
    "# make a work directory\n",
    "try:\n",
    "    os.mkdir(\"Work/\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#go through the files in files\n",
    "if os.path.isfile(\"Work/wordsChinese.txt\") == False:\n",
    "    for file in files:\n",
    "        #print out the name of the file that is being processed now\n",
    "        print(file)\n",
    "        #check if the file is a txt file\n",
    "        if file.endswith(\".txt\"):\n",
    "            #if txt file satisfied, open it and read it into the text space\n",
    "            text = open(path + file, errors = 'ignore', encoding = 'utf-8').read() + '\\n\\n'\n",
    "            #preprocessing: remove the punctuations\n",
    "            puncs = punctuation + u'.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "            text = re.sub(r\"[{}]+\".format(puncs),\" \",text)\n",
    "\n",
    "            #preprocessing: remove the numbers \n",
    "            text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "\n",
    "            #preprocessing: word segmentation\n",
    "            #we use jieba library and the lcut function to get the word tokens\n",
    "            tokens = jieba.lcut(text)\n",
    "\n",
    "            #make cuts to get the part of speech for each word\n",
    "            cuts = pseg.cut(text)   \n",
    "            #get the list of stopwords\n",
    "            #the stopwords txt file downloaded from github is included in our file submission\n",
    "            #the following path is where you stop the stopwords txt file\n",
    "            path2 = r'StopwordsChinese.txt'\n",
    "            #get the list of stopwords\n",
    "            stopwords = [line.strip() for line in open(path2, encoding=\"utf-8\").readlines()]\n",
    "            #create a list to store the words\n",
    "            words = []\n",
    "            for word, flag in cuts:\n",
    "                #print(flag)\n",
    "                #if the word is not proper noun and not a stopword, insert it into the words list\n",
    "                if flag != ('ns' or 'nz' or 'nr' or 'nrt'):\n",
    "                    if word not in stopwords:\n",
    "                        words.append(word)\n",
    "            #get the final text by inserting the words in the word list in our final text\n",
    "            text_final = \"\"\n",
    "            for i in words:\n",
    "                text_final += i\n",
    "                text_final += \" \"\n",
    "            #open a new file and write the words into it\n",
    "            #change the path to your own path where you want to store the words.txt file \n",
    "            with open (\"Work/wordsChinese.txt\", 'a', encoding = 'utf-8') as f:\n",
    "                #a single line of words for each file \n",
    "                text_write = text_final + '\\n'\n",
    "                #write the words into the file\n",
    "                f.write(text_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
